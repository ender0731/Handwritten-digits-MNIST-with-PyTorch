{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the environment\n",
    "import torch  \n",
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np  \n",
    "import torch.nn.functional as func  \n",
    "from torch import nn  \n",
    "from torchvision import datasets,transforms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying transforms (converting to tensor; normalizing) on our training and validation datasets\n",
    "transform1=transforms.Compose([transforms.Resize((28,28)),transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])  \n",
    "training_dataset=datasets.MNIST(root='./data',train=True,download=True,transform=transform1)  \n",
    "validation_dataset=datasets.MNIST(root='./data',train=False,download=True,transform=transform1)  \n",
    "training_loader=torch.utils.data.DataLoader(dataset=training_dataset,batch_size=100,shuffle=True) # Shuffling training data \n",
    "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=100,shuffle=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a class object to define our neural network of two hidden layers; note that relu was used for the activation functions\n",
    "class classification1(nn.Module):  \n",
    "    def __init__(self,input_layer,hidden_layer1,hidden_layer2,output_layer):  \n",
    "        super().__init__()  \n",
    "        self.linear1=nn.Linear(input_layer,hidden_layer1)  \n",
    "        self.linear2=nn.Linear(hidden_layer1,hidden_layer2)  \n",
    "        self.linear3=nn.Linear(hidden_layer2,output_layer)  \n",
    "    def forward(self,x):  \n",
    "        x=func.relu(self.linear1(x))  \n",
    "        x=func.relu(self.linear2(x))  \n",
    "        x=self.linear3(x)  \n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the network with 10 neurons per hidden layer\n",
    "#Loss function used was cross-entropy; a combo of NLLLoss and log_softmax\n",
    "model=classification1(784,10,10,10)  \n",
    "criteron=nn.CrossEntropyLoss()  \n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.0001)  \n",
    "epochs=10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:1.1629,68.4067\n",
      "training_loss:0.8244,77.1867\n",
      "training_loss:0.6572,81.6017\n",
      "training_loss:0.5683,83.8167\n",
      "training_loss:0.5151,85.1167\n",
      "training_loss:0.4790,86.1467\n",
      "training_loss:0.4529,86.8067\n",
      "training_loss:0.4334,87.3683\n",
      "training_loss:0.4173,87.9150\n",
      "training_loss:0.4051,88.2517\n"
     ]
    }
   ],
   "source": [
    "#Running the model for 10 epochs and observing the model accuracy\n",
    "for e in range(epochs):  \n",
    "    loss=0.0  \n",
    "    correct=0.0  \n",
    "    for input,labels in training_loader:  \n",
    "        inputs=input.view(input.shape[0],-1)  \n",
    "        outputs=model(inputs)  \n",
    "        loss1=criteron(outputs,labels)  \n",
    "        optimizer.zero_grad()  \n",
    "        loss1.backward()  \n",
    "        optimizer.step()  \n",
    "        _,preds=torch.max(outputs,1)  \n",
    "        loss+=loss1.item()  \n",
    "        correct+=torch.sum(preds==labels.data)  \n",
    "    else:  \n",
    "        epoch_loss=loss/len(training_loader)  \n",
    "        epoch_acc=correct.float()/len(training_loader)  \n",
    "        print('training_loss:{:.4f},{:.4f}'.format(epoch_loss,epoch_acc.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that increasing the number of neurons would increase the number of tuneable parameters, making the model\n",
    "# more sensitive to the training data. However, this comes at a computational cost and also may result in overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:0.8391,80.3667\n",
      "training_loss:0.3421,90.1717\n",
      "training_loss:0.2863,91.6767\n",
      "training_loss:0.2494,92.7517\n",
      "training_loss:0.2206,93.6467\n",
      "training_loss:0.1976,94.2583\n",
      "training_loss:0.1792,94.8150\n",
      "training_loss:0.1624,95.3050\n",
      "training_loss:0.1486,95.6183\n",
      "training_loss:0.1367,95.9883\n"
     ]
    }
   ],
   "source": [
    "# Building a model with 150 nuerons for each hidden layer\n",
    "model=classification1(784,150,150,10)  \n",
    "criteron=nn.CrossEntropyLoss()  \n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.0001)  \n",
    "epochs=10   \n",
    "for e in range(epochs):  \n",
    "    loss=0.0  \n",
    "    correct=0.0  \n",
    "    for input,labels in training_loader:  \n",
    "        inputs=input.view(input.shape[0],-1)  \n",
    "        outputs=model(inputs)  \n",
    "        loss1=criteron(outputs,labels)  \n",
    "        optimizer.zero_grad()  \n",
    "        loss1.backward()  \n",
    "        optimizer.step()  \n",
    "        _,preds=torch.max(outputs,1)  \n",
    "        loss+=loss1.item()  \n",
    "        correct+=torch.sum(preds==labels.data)  \n",
    "    else:  \n",
    "        epoch_loss=loss/len(training_loader)  \n",
    "        epoch_acc=correct.float()/len(training_loader)  \n",
    "        print('training_loss:{:.4f},{:.4f}'.format(epoch_loss,epoch_acc.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got a training accuracy of 96 percent here as opposed to a simpler network's accuracy of a 88 percent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
